"""
Training Script - FIXED VERSION
Critical Fix: Apply SMOTE ONLY on training data AFTER train/test split
This prevents Data Leakage and gives honest evaluation metrics
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
from imblearn.over_sampling import SMOTE
import joblib

# Create models directory
os.makedirs('models', exist_ok=True)

print("=" * 70)
print("üöÄ TELCO CUSTOMER CHURN - MODEL TRAINING (FIXED VERSION)")
print("=" * 70)
print("‚ö†Ô∏è  FIX: SMOTE applied ONLY on TRAINING data to prevent Data Leakage")
print("=" * 70)

# Load data
print("\nüìä Loading data...")
df = pd.read_csv('data/WA_Fn-UseC_-Telco-Customer-Churn.csv')
print(f"   Dataset shape: {df.shape}")

# ============== DATA CLEANING ==============
print("\nüßπ Cleaning data...")
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'] = df['TotalCharges'].fillna(0)
print("   ‚úÖ TotalCharges converted to numeric")

# ============== FEATURE ENGINEERING ==============
print("\nüîß Feature Engineering...")

def get_tenure_group(tenure):
    if tenure <= 12:
        return '0-12'
    elif tenure <= 24:
        return '12-24'
    elif tenure <= 48:
        return '24-48'
    elif tenure <= 72:
        return '48-72'
    else:
        return '72+'

df['Tenure_Group'] = df['tenure'].apply(get_tenure_group)

service_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 
                'TechSupport', 'StreamingTV', 'StreamingMovies']
df['Number_of_Services'] = df[service_cols].apply(
    lambda row: sum(row == 'Yes'), axis=1
)
print("   ‚úÖ Created Tenure_Group and Number_of_Services")

# ============== PREPROCESSING ==============
print("\n‚öôÔ∏è Preprocessing...")

# Separate features and target
y = df['Churn'].copy()
X = df.drop(columns=['Churn', 'customerID'])

# Encode target
le_target = LabelEncoder()
y_encoded = le_target.fit_transform(y)

# Binary columns to encode
binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']
for col in binary_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

# Categorical columns for one-hot encoding
cat_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
            'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
            'Contract', 'PaymentMethod', 'Tenure_Group']

# Numeric columns
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges', 'SeniorCitizen', 
            'gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling',
            'Number_of_Services']

# Create preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_cols)
    ],
    remainder='drop'
)

X_processed = preprocessor.fit_transform(X)
print(f"   Processed features shape: {X_processed.shape}")

# ============== CRITICAL FIX: SPLIT FIRST, THEN SMOTE ==============
print("\n" + "=" * 70)
print("‚ö†Ô∏è  CRITICAL: Splitting data FIRST, then applying SMOTE only to TRAIN")
print("=" * 70)

# STEP 1: Split data FIRST (stratified to maintain class ratio)
X_train, X_test, y_train, y_test = train_test_split(
    X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print(f"\nüìä BEFORE SMOTE:")
print(f"   Train set: {X_train.shape[0]} samples")
print(f"   Test set:  {X_test.shape[0]} samples")
print(f"   Train class distribution: No={sum(y_train==0)}, Yes={sum(y_train==1)} ({sum(y_train==1)/len(y_train)*100:.1f}% Churn)")
print(f"   Test class distribution:  No={sum(y_test==0)}, Yes={sum(y_test==1)} ({sum(y_test==1)/len(y_test)*100:.1f}% Churn)")

# STEP 2: Apply SMOTE ONLY to TRAINING data
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(f"\nüìä AFTER SMOTE (only on TRAIN):")
print(f"   Train set (SMOTE): {X_train_smote.shape[0]} samples")
print(f"   Train class distribution: No={sum(y_train_smote==0)}, Yes={sum(y_train_smote==1)}")
print(f"   Test set (UNCHANGED): {X_test.shape[0]} samples")  
print(f"   Test class distribution:  No={sum(y_test==0)}, Yes={sum(y_test==1)} ({sum(y_test==1)/len(y_test)*100:.1f}% Churn)")
print("\n   ‚úÖ Test set maintains NATURAL imbalanced distribution!")

# ============== MODEL TRAINING ==============
print("\nü§ñ Training Models...")

results = []

# 1. Logistic Regression
print("\n   1. Logistic Regression...")
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train_smote, y_train_smote)
y_pred_lr = lr.predict(X_test)
y_prob_lr = lr.predict_proba(X_test)[:, 1]

results.append({
    'Model': 'Logistic Regression',
    'Accuracy': accuracy_score(y_test, y_pred_lr),
    'Precision': precision_score(y_test, y_pred_lr),
    'Recall': recall_score(y_test, y_pred_lr),
    'F1-Score': f1_score(y_test, y_pred_lr),
    'ROC-AUC': roc_auc_score(y_test, y_prob_lr)
})
print(f"      Accuracy: {results[-1]['Accuracy']:.4f}, F1-Score: {results[-1]['F1-Score']:.4f}")
joblib.dump(lr, 'models/logistic_regression.pkl')

# 2. Random Forest
print("\n   2. Random Forest...")
rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
rf.fit(X_train_smote, y_train_smote)
y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:, 1]

results.append({
    'Model': 'Random Forest',
    'Accuracy': accuracy_score(y_test, y_pred_rf),
    'Precision': precision_score(y_test, y_pred_rf),
    'Recall': recall_score(y_test, y_pred_rf),
    'F1-Score': f1_score(y_test, y_pred_rf),
    'ROC-AUC': roc_auc_score(y_test, y_prob_rf)
})
print(f"      Accuracy: {results[-1]['Accuracy']:.4f}, F1-Score: {results[-1]['F1-Score']:.4f}")
joblib.dump(rf, 'models/random_forest.pkl')

# 3. XGBoost
print("\n   3. XGBoost...")
xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, 
                    random_state=42, eval_metric='logloss')
xgb.fit(X_train_smote, y_train_smote)
y_pred_xgb = xgb.predict(X_test)
y_prob_xgb = xgb.predict_proba(X_test)[:, 1]

results.append({
    'Model': 'XGBoost',
    'Accuracy': accuracy_score(y_test, y_pred_xgb),
    'Precision': precision_score(y_test, y_pred_xgb),
    'Recall': recall_score(y_test, y_pred_xgb),
    'F1-Score': f1_score(y_test, y_pred_xgb),
    'ROC-AUC': roc_auc_score(y_test, y_prob_xgb)
})
print(f"      Accuracy: {results[-1]['Accuracy']:.4f}, F1-Score: {results[-1]['F1-Score']:.4f}")
joblib.dump(xgb, 'models/xgboost.pkl')

# 4. Neural Network
print("\n   4. Neural Network (Deep Learning)...")
try:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout
    from tensorflow.keras.callbacks import EarlyStopping
    
    nn = Sequential([
        Dense(16, activation='relu', input_dim=X_train_smote.shape[1]),
        Dropout(0.3),
        Dense(8, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    nn.fit(X_train_smote, y_train_smote, epochs=100, batch_size=32, 
           validation_split=0.2, callbacks=[early_stop], verbose=0)
    
    y_prob_nn = nn.predict(X_test, verbose=0).flatten()
    y_pred_nn = (y_prob_nn > 0.5).astype(int)
    
    results.append({
        'Model': 'Neural Network',
        'Accuracy': accuracy_score(y_test, y_pred_nn),
        'Precision': precision_score(y_test, y_pred_nn),
        'Recall': recall_score(y_test, y_pred_nn),
        'F1-Score': f1_score(y_test, y_pred_nn),
        'ROC-AUC': roc_auc_score(y_test, y_prob_nn)
    })
    print(f"      Accuracy: {results[-1]['Accuracy']:.4f}, F1-Score: {results[-1]['F1-Score']:.4f}")
    nn.save('models/neural_network.h5')
except Exception as e:
    print(f"      ‚ö†Ô∏è Neural Network skipped: {e}")

# ============== RESULTS ==============
print("\n" + "=" * 70)
print("üìä MODEL COMPARISON RESULTS (HONEST - No Data Leakage)")
print("=" * 70)

results_df = pd.DataFrame(results).round(4)
results_df = results_df.sort_values('F1-Score', ascending=False)
print(results_df.to_string(index=False))

# Save results
results_df.to_csv('models/model_results.csv', index=False)

# Select best model
best_model_name = results_df.iloc[0]['Model']
best_f1 = results_df.iloc[0]['F1-Score']
print(f"\nüèÜ Best Model: {best_model_name} (F1-Score: {best_f1:.4f})")

# Save best model
best_models = {
    'Logistic Regression': lr,
    'Random Forest': rf,
    'XGBoost': xgb
}
if best_model_name in best_models:
    best_model = best_models[best_model_name]
else:
    best_model = xgb  # Default to XGBoost
    
joblib.dump(best_model, 'models/best_model.pkl')
joblib.dump(preprocessor, 'models/preprocessor.pkl')
joblib.dump(le_target, 'models/label_encoder.pkl')

# ============== ERROR ANALYSIS ==============
print("\n" + "=" * 70)
print("üîç ERROR ANALYSIS (False Negatives - Missed Churners)")
print("=" * 70)

# Get False Negatives from best model
if best_model_name == 'XGBoost':
    best_pred = y_pred_xgb
elif best_model_name == 'Random Forest':
    best_pred = y_pred_rf
else:
    best_pred = y_pred_lr

# Confusion matrix details
cm = confusion_matrix(y_test, best_pred)
tn, fp, fn, tp = cm.ravel()

print(f"\nConfusion Matrix Breakdown:")
print(f"   True Negatives (TN):  {tn} - Correctly predicted as NOT churning")
print(f"   False Positives (FP): {fp} - Predicted to churn but didn't")
print(f"   False Negatives (FN): {fn} - MISSED! Predicted to stay but churned")
print(f"   True Positives (TP):  {tp} - Correctly predicted as churning")

print(f"\nüìä Test Set Class Distribution (Verifying No Leakage):")
print(f"   Total Test Samples: {len(y_test)}")
print(f"   No Churn (0): {sum(y_test==0)} ({sum(y_test==0)/len(y_test)*100:.1f}%)")
print(f"   Yes Churn (1): {sum(y_test==1)} ({sum(y_test==1)/len(y_test)*100:.1f}%)")
print(f"   ‚úÖ Test set maintains natural ~27% churn ratio!")

# ============== VISUALIZATIONS ==============
print("\nüìà Generating result visualizations...")

# Model comparison bar chart
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Metrics comparison
x = np.arange(len(results_df))
width = 0.15
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']

for i, metric in enumerate(metrics):
    axes[0].bar(x + i*width, results_df[metric], width, label=metric)

axes[0].set_xticks(x + width * 2)
axes[0].set_xticklabels(results_df['Model'], rotation=15)
axes[0].set_ylabel('Score')
axes[0].set_title('Model Performance Comparison (No Data Leakage)', fontweight='bold')
axes[0].legend(loc='lower right')
axes[0].set_ylim(0.3, 1.0)

# ROC Curves
models_for_roc = [
    ('Logistic Regression', y_prob_lr),
    ('Random Forest', y_prob_rf),
    ('XGBoost', y_prob_xgb),
]
if 'y_prob_nn' in dir():
    models_for_roc.append(('Neural Network', y_prob_nn))

for name, y_prob in models_for_roc:
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    auc = roc_auc_score(y_test, y_prob)
    axes[1].plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')

axes[1].plot([0, 1], [0, 1], 'k--', label='Random')
axes[1].set_xlabel('False Positive Rate')
axes[1].set_ylabel('True Positive Rate')
axes[1].set_title('ROC Curves', fontweight='bold')
axes[1].legend()

plt.tight_layout()
plt.savefig('models/model_comparison.png', dpi=150, bbox_inches='tight')
plt.close()
print("   ‚úÖ Saved model_comparison.png")

# Confusion matrices
fig, axes = plt.subplots(2, 2, figsize=(10, 8))

models_for_cm = [
    ('Logistic Regression', y_pred_lr),
    ('Random Forest', y_pred_rf),
    ('XGBoost', y_pred_xgb),
]
if 'y_pred_nn' in dir():
    models_for_cm.append(('Neural Network', y_pred_nn))
else:
    models_for_cm.append(('XGBoost', y_pred_xgb))

for idx, (name, y_pred) in enumerate(models_for_cm):
    ax = axes[idx // 2, idx % 2]
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=['No Churn', 'Churn'],
                yticklabels=['No Churn', 'Churn'])
    ax.set_title(name)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')

plt.suptitle('Confusion Matrices (Honest Evaluation)', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('models/confusion_matrices.png', dpi=150, bbox_inches='tight')
plt.close()
print("   ‚úÖ Saved confusion_matrices.png")

# Feature importance
try:
    num_features = num_cols
    cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols).tolist()
    feature_names = num_features + cat_features
    
    importance_df = pd.DataFrame({
        'Feature': feature_names[:len(rf.feature_importances_)],
        'Importance': rf.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    plt.figure(figsize=(10, 6))
    top_features = importance_df.head(15)
    plt.barh(range(len(top_features)), top_features['Importance'], color='#3498db')
    plt.yticks(range(len(top_features)), top_features['Feature'])
    plt.xlabel('Importance')
    plt.title('Random Forest - Top 15 Feature Importances')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('models/feature_importance.png', dpi=150, bbox_inches='tight')
    plt.close()
    print("   ‚úÖ Saved feature_importance.png")
except Exception as e:
    print(f"   ‚ö†Ô∏è Feature importance plot skipped: {e}")

# ============== SUMMARY ==============
print("\n" + "=" * 70)
print("‚úÖ TRAINING COMPLETE (HONEST RESULTS - No Data Leakage)")
print("=" * 70)
print("\nüìã Key Differences from Previous (Leaked) Version:")
print("   - Train/Test split done BEFORE SMOTE")
print("   - SMOTE applied ONLY to training data")
print("   - Test set maintains natural ~27% churn ratio")
print("   - Results are LOWER but HONEST (real-world performance)")
print("\nüìù Note on Deep Learning Performance:")
print("   Neural Network underperforms tree-based models on tabular data.")
print("   This is expected: XGBoost/RF handle categorical features better")
print("   and require less data to converge than neural networks.")
